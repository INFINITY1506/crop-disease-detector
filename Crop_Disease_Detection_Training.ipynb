{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸŒ¿ Crop Disease Detection Model Training\n",
        "Complete notebook to train a crop disease detection model compatible with your Streamlit app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow==2.16.0 keras pillow matplotlib seaborn scikit-learn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 15 classes for your Streamlit app\n",
        "CLASS_NAMES = [\n",
        "    \"Pepper__bell___Bacterial_spot\",\n",
        "    \"Pepper__bell___healthy\", \n",
        "    \"Potato___Early_blight\",\n",
        "    \"Potato___Late_blight\",\n",
        "    \"Potato___healthy\",\n",
        "    \"Tomato_Bacterial_spot\",\n",
        "    \"Tomato_Early_blight\", \n",
        "    \"Tomato_Late_blight\",\n",
        "    \"Tomato_Leaf_Mold\",\n",
        "    \"Tomato_Septoria_leaf_spot\",\n",
        "    \"Tomato_Spider_mites_Two_spotted_spider_mite\",\n",
        "    \"Tomato__Target_Spot\",\n",
        "    \"Tomato__Tomato_YellowLeaf__Curl_Virus\",\n",
        "    \"Tomato__Tomato_mosaic_virus\",\n",
        "    \"Tomato_healthy\"\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "print(f\"ğŸ“Š Number of classes: {NUM_CLASSES}\")\n",
        "class_indices = {str(i): class_name for i, class_name in enumerate(CLASS_NAMES)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload your dataset (ZIP file with folders for each class)\n",
        "print(\"ğŸ“¤ Upload your dataset ZIP file\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('dataset')\n",
        "        print(f\"âœ… Extracted {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    'dataset',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=CLASS_NAMES\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    'dataset',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\", \n",
        "    seed=42,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=CLASS_NAMES\n",
        ")\n",
        "\n",
        "print(\"âœ… Dataset loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "    layers.RandomBrightness(0.1)\n",
        "])\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare_dataset(ds, augment=False):\n",
        "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    return ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds = prepare_dataset(train_ds, augment=True)\n",
        "val_ds = prepare_dataset(val_ds, augment=False)\n",
        "print(\"âœ… Data preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the model\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=IMG_SIZE + (3,)\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=IMG_SIZE + (3,)),\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"ğŸš€ Starting training...\")\n",
        "EPOCHS = 25\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"âœ… Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export model for Streamlit deployment\n",
        "print(\"ğŸ“¦ Exporting model for Streamlit...\")\n",
        "\n",
        "os.makedirs('streamlit_models', exist_ok=True)\n",
        "\n",
        "# Save model in multiple formats\n",
        "model.save('streamlit_models/model_savedmodel', save_format='tf')\n",
        "model.save('streamlit_models/model_new.keras')\n",
        "model.save('streamlit_models/model.h5')\n",
        "\n",
        "# Save class indices\n",
        "with open('streamlit_models/class_indices.json', 'w') as f:\n",
        "    json.dump(class_indices, f, indent=2)\n",
        "\n",
        "print(\"âœ… Model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create disease info CSV\n",
        "disease_info_data = []\n",
        "for class_name in CLASS_NAMES:\n",
        "    if 'healthy' in class_name.lower():\n",
        "        disease_info_data.append({\n",
        "            'label': class_name,\n",
        "            'title': 'Healthy Plant',\n",
        "            'description': 'No visible disease symptoms. Leaves are green and healthy.',\n",
        "            'treatment': 'Maintain routine nutrition and irrigation.',\n",
        "            'prevention': 'Use certified seeds, rotate crops, practice good sanitation.',\n",
        "            'reference': 'Standard agricultural practices'\n",
        "        })\n",
        "    else:\n",
        "        parts = class_name.split('_')\n",
        "        crop = parts[0]\n",
        "        disease = '_'.join(parts[1:]) if len(parts) > 1 else 'Disease'\n",
        "        \n",
        "        disease_info_data.append({\n",
        "            'label': class_name,\n",
        "            'title': f'{crop} {disease}',\n",
        "            'description': f'Disease affecting {crop} plants requiring attention.',\n",
        "            'treatment': 'Consult agricultural extension services.',\n",
        "            'prevention': 'Use disease-resistant varieties, proper spacing.',\n",
        "            'reference': 'Agricultural disease management guidelines'\n",
        "        })\n",
        "\n",
        "disease_info_df = pd.DataFrame(disease_info_data)\n",
        "disease_info_df.to_csv('streamlit_models/disease_info.csv', index=False)\n",
        "print(\"âœ… Disease info created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create download package\n",
        "import shutil\n",
        "shutil.make_archive('crop_disease_model_complete', 'zip', 'streamlit_models')\n",
        "\n",
        "print(\"ğŸ‰ Model export complete!\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy:.1%}\")\n",
        "print(\"\\nğŸ“ Files ready for download:\")\n",
        "print(\"   â”œâ”€â”€ model_savedmodel/ (Primary model)\")\n",
        "print(\"   â”œâ”€â”€ model_new.keras (Backup)\")\n",
        "print(\"   â”œâ”€â”€ model.h5 (Alternative)\")\n",
        "print(\"   â”œâ”€â”€ class_indices.json\")\n",
        "print(\"   â””â”€â”€ disease_info.csv\")\n",
        "\n",
        "files.download('crop_disease_model_complete.zip')\n",
        "\n",
        "print(\"\\nğŸš€ DEPLOYMENT INSTRUCTIONS:\")\n",
        "print(\"1. Download the ZIP file\")\n",
        "print(\"2. Extract and copy files to your Streamlit app's models/ folder\")\n",
        "print(\"3. Commit and push to GitHub\")\n",
        "print(\"4. Your app will automatically use the trained model!\")\n",
        "print(\"\\nâœ… Your real AI model is ready for deployment! ğŸŒ¿\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
