{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸŒ¿ Fresh Crop Disease Detection Training - Zero Errors Guaranteed!\n",
        "## Complete notebook that works from start to finish with PlantVillage dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install all required libraries\n",
        "print(\"ğŸ“¦ Installing required libraries...\")\n",
        "!pip install tensorflow==2.16.0 -q\n",
        "!pip install keras -q\n",
        "!pip install pillow -q\n",
        "!pip install matplotlib -q\n",
        "!pip install seaborn -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install kaggle -q\n",
        "\n",
        "# Import all libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
        "print(f\"âœ… Keras version: {tf.keras.__version__}\")\n",
        "print(\"âœ… All libraries installed successfully!\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Download PlantVillage dataset from Kaggle (properly structured)\n",
        "print(\"ğŸ“¤ Please upload your kaggle.json file\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Set up Kaggle API\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!chmod 600 /content/kaggle.json\n",
        "\n",
        "print(\"ğŸ“¥ Downloading PlantVillage dataset...\")\n",
        "!kaggle datasets download -d abdallahalidev/plantvillage-dataset\n",
        "\n",
        "print(\"ğŸ“¦ Extracting dataset...\")\n",
        "with zipfile.ZipFile('plantvillage-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "# Find the dataset directory\n",
        "dataset_found = False\n",
        "possible_names = ['PlantVillage', 'plantvillage', 'color', 'PlantVillage-Dataset']\n",
        "\n",
        "for name in possible_names:\n",
        "    if os.path.exists(name):\n",
        "        # Check if it has class subdirectories\n",
        "        subdirs = [d for d in os.listdir(name) if os.path.isdir(os.path.join(name, d))]\n",
        "        if len(subdirs) > 10:  # Should have many classes\n",
        "            if name != 'dataset':\n",
        "                if os.path.exists('dataset'):\n",
        "                    shutil.rmtree('dataset')\n",
        "                shutil.move(name, 'dataset')\n",
        "            dataset_found = True\n",
        "            break\n",
        "\n",
        "# If not found in common names, search all directories\n",
        "if not dataset_found:\n",
        "    for item in os.listdir('.'):\n",
        "        if os.path.isdir(item) and item not in ['.config', '__pycache__']:\n",
        "            try:\n",
        "                subdirs = [d for d in os.listdir(item) if os.path.isdir(os.path.join(item, d))]\n",
        "                if len(subdirs) > 10:\n",
        "                    if item != 'dataset':\n",
        "                        if os.path.exists('dataset'):\n",
        "                            shutil.rmtree('dataset')\n",
        "                        shutil.move(item, 'dataset')\n",
        "                    dataset_found = True\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "if dataset_found:\n",
        "    # Show available classes\n",
        "    all_classes = [d for d in os.listdir('dataset') if os.path.isdir(os.path.join('dataset', d))]\n",
        "    print(f\"âœ… Dataset extracted successfully!\")\n",
        "    print(f\"ğŸ“Š Found {len(all_classes)} disease classes:\")\n",
        "    \n",
        "    for i, class_name in enumerate(sorted(all_classes)[:20]):  # Show first 20\n",
        "        class_path = os.path.join('dataset', class_name)\n",
        "        img_count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"   {i+1:2d}. {class_name}: {img_count} images\")\n",
        "    \n",
        "    if len(all_classes) > 20:\n",
        "        print(f\"   ... and {len(all_classes)-20} more classes\")\n",
        "    \n",
        "    print(\"\\nğŸ¯ Ready for training!\")\n",
        "else:\n",
        "    print(\"âŒ Could not find dataset. Please check the download.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
